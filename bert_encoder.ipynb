{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5P5SeObBKQJptPUjZcq2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgreed4/no_hate_transformer/blob/kgreed/bert_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('cleaned_data_sw.csv')\n",
        "\n",
        "# Drop every column that isn't tweet or class\n",
        "df = df.drop(df.columns.difference(['tweet', 'class']), axis=1)\n",
        "\n",
        "# First, we want to use the BERT tokenizer to tokenize and encode the dataset into embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# We will create a small function that will take care of tokenization and encoding\n",
        "def encode_texts(tokenizer, texts, max_length):\n",
        "    encoding = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        # This is required to add special tokens such as the [CLS] and [SEP] tokens that indicate the start and end of a sentence\n",
        "        add_special_tokens=True,\n",
        "        # Here the padding variable is responsible for padding the sequences to the same length\n",
        "        padding='max_length',\n",
        "        # The max length of the tokenized sequences\n",
        "        max_length=max_length,\n",
        "        return_attention_mask=True,\n",
        "        # Here we specify that we want the output to be TensorFlow tensors\n",
        "        return_tensors='tf',\n",
        "        # If the sequence is longer than max_length, it will be truncated to a fixed length\n",
        "        truncation=True\n",
        "    )\n",
        "    # The encoding['input_ids'] contains the tokenized sequences\n",
        "    # The encoding['attention_mask'] contains the attention masks and tells the model which tokens to pay attention to and which ones to ignore (mask token)\n",
        "    return encoding['input_ids'], encoding['attention_mask']\n",
        "\n",
        "# Here we define the maximum length (randomly chosen per ChatGPT's recommendation)\n",
        "max_length = 128\n",
        "\n",
        "# We can then call the function to tokenize and encode the dataset\n",
        "input_ids, attention_masks = encode_texts(tokenizer, df['tweet'].tolist(), max_length)\n",
        "\n",
        "# Here we create labels from the 'class' column\n",
        "# This is the target variable that we want to predict\n",
        "labels = tf.convert_to_tensor(df['class'].values, dtype=tf.int32).numpy()\n",
        "\n",
        "# For some reason, I was getting an error saying that I needed to convert to NumPy arrays instead of TensorFlow tensors\n",
        "# So I converted the input_ids and attention_masks to NumPy arrays\n",
        "input_ids_np = input_ids.numpy()\n",
        "attention_masks_np = attention_masks.numpy()\n",
        "\n",
        "# We can then use the train_test_split function from scikit-learn to split the dataset into training and validation sets\n",
        "train_inputs_np, validation_inputs_np, train_labels_np, validation_labels_np = train_test_split(input_ids_np, labels, random_state=2021, test_size=0.1)\n",
        "train_masks_np, validation_masks_np, _, _ = train_test_split(attention_masks_np, labels, random_state=2021, test_size=0.1)\n",
        "\n",
        "# Here the BUFFER_SIZE is the number of training inputs\n",
        "BUFFER_SIZE = len(train_inputs_np)\n",
        "# The batch size is 32\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# We can then take the NumPy arrays and convert them to TensorFlow datasets for both the training and validation sets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_inputs_np, train_masks_np), train_labels_np)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(((validation_inputs_np, validation_masks_np), validation_labels_np)).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "l7VSNqKTF1dG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=2):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        for inputs, attention_masks, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, attention_mask=attention_masks)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    return model, train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "5TpS0R9eHJdf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 512\n",
        "input_vocab_size = tokenizer.vocab_size + 2\n",
        "maximum_position_encoding = 512\n",
        "rate = 0.1\n",
        "num_classes = df['class'].nunique()\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Fine-tune BERT model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, train_accuracies = train_model(model, train_loader, criterion, optimizer, num_epochs=2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(train_accuracies)\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(train_losses)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjLr31sjGEmr",
        "outputId": "0a6696b5-86ad-4dee-e5f9-35c7c97346c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}