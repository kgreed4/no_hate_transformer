{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgreed4/no_hate_transformer/blob/kgreed/bert_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czcno_qCIbyj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7VSNqKTF1dG"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('cleaned_data_sw.csv')\n",
        "\n",
        "# Drop every column that isn't tweet or class\n",
        "df = df.drop(df.columns.difference(['tweet', 'class']), axis=1)\n",
        "\n",
        "# First, we want to use the BERT tokenizer to tokenize and encode the dataset into embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# We will create a small function that will take care of tokenization and encoding\n",
        "def encode_texts(tokenizer, texts, max_length):\n",
        "    encoding = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        # This is required to add special tokens such as the [CLS] and [SEP] tokens that indicate the start and end of a sentence\n",
        "        add_special_tokens=True,\n",
        "        # Here the padding variable is responsible for padding the sequences to the same length\n",
        "        padding='max_length',\n",
        "        # The max length of the tokenized sequences\n",
        "        max_length=max_length,\n",
        "        return_attention_mask=True,\n",
        "        # Here we specify that we want the output to be TensorFlow tensors\n",
        "        return_tensors='tf',\n",
        "        # If the sequence is longer than max_length, it will be truncated to a fixed length\n",
        "        truncation=True\n",
        "    )\n",
        "    # The encoding['input_ids'] contains the tokenized sequences\n",
        "    # The encoding['attention_mask'] contains the attention masks and tells the model which tokens to pay attention to and which ones to ignore (mask token)\n",
        "    return encoding['input_ids'], encoding['attention_mask']\n",
        "\n",
        "# Here we define the maximum length (randomly chosen per ChatGPT's recommendation)\n",
        "max_length = 128\n",
        "\n",
        "# We can then call the function to tokenize and encode the dataset\n",
        "input_ids, attention_masks = encode_texts(tokenizer, df['tweet'].tolist(), max_length)\n",
        "\n",
        "# Here we create labels from the 'class' column\n",
        "# This is the target variable that we want to predict\n",
        "labels = tf.convert_to_tensor(df['class'].values, dtype=tf.int32).numpy()\n",
        "\n",
        "# For some reason, I was getting an error saying that I needed to convert to NumPy arrays instead of TensorFlow tensors\n",
        "# So I converted the input_ids and attention_masks to NumPy arrays\n",
        "input_ids_np = input_ids.numpy()\n",
        "attention_masks_np = attention_masks.numpy()\n",
        "\n",
        "# We can then use the train_test_split function from scikit-learn to split the dataset into training and validation sets\n",
        "train_inputs_np, validation_inputs_np, train_labels_np, validation_labels_np = train_test_split(input_ids_np, labels, random_state=2021, test_size=0.1)\n",
        "train_masks_np, validation_masks_np, _, _ = train_test_split(attention_masks_np, labels, random_state=2021, test_size=0.1)\n",
        "\n",
        "# Here the BUFFER_SIZE is the number of training inputs\n",
        "BUFFER_SIZE = len(train_inputs_np)\n",
        "# The batch size is 32\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# We can then take the NumPy arrays and convert them to TensorFlow datasets for both the training and validation sets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_inputs_np, train_masks_np), train_labels_np)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(((validation_inputs_np, validation_masks_np), validation_labels_np)).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX69CBkhNPNc",
        "outputId": "d90bf98e-ea4f-4c6f-8d02-a7329055e19c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "697/697 [==============================] - 32704s 47s/step - loss: 0.4638 - accuracy: 0.8300 - val_loss: 0.4767 - val_accuracy: 0.8217\n",
            "Epoch 2/2\n",
            "697/697 [==============================] - 32327s 46s/step - loss: 0.4541 - accuracy: 0.8332 - val_loss: 0.4695 - val_accuracy: 0.8217\n",
            "697/697 [==============================] - 9635s 14s/step - loss: 0.4508 - accuracy: 0.8332\n",
            "Training Accuracy: 0.8331689238548279\n",
            "Training Loss: 0.45084381103515625\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def train_bert_sequence_classification(train_dataset, validation_dataset, num_epochs=2):\n",
        "    # Initialize the BERT model for sequence classification\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "\n",
        "    # Define the loss function\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n",
        "\n",
        "    # Evaluate the model on training dataset\n",
        "    train_loss, train_accuracy = model.evaluate(train_dataset)\n",
        "    print(f\"Training Accuracy: {train_accuracy}\")\n",
        "    print(f\"Training Loss: {train_loss}\")\n",
        "\n",
        "# Train the BERT model\n",
        "train_bert_sequence_classification(train_dataset, validation_dataset)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx8T0MrLmCexzOtk+3brid",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}