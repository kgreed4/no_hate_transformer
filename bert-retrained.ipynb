{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNx8T0MrLmCexzOtk+3brid","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/kgreed4/no_hate_transformer/blob/kgreed/bert_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport torch\nfrom transformers import BertForSequenceClassification\nimport matplotlib.pyplot as plt","metadata":{"id":"Czcno_qCIbyj","execution":{"iopub.status.busy":"2024-03-22T18:40:12.804129Z","iopub.execute_input":"2024-03-22T18:40:12.804523Z","iopub.status.idle":"2024-03-22T18:40:32.675625Z","shell.execute_reply.started":"2024-03-22T18:40:12.804494Z","shell.execute_reply":"2024-03-22T18:40:32.674811Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-22 18:40:15.609556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-22 18:40:15.609668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-22 18:40:15.759324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/no-hate-transformer/cleaned_data_nosw.csv')\n\n# Convert to string\ndf['tweet'] = str(df['tweet'])\n\n# Drop every column that isn't tweet or class\ndf = df.drop(df.columns.difference(['tweet', 'class']), axis=1)\n\n# First, we want to use the BERT tokenizer to tokenize and encode the dataset into embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# We will create a small function that will take care of tokenization and encoding\ndef encode_texts(tokenizer, texts, max_length):\n    encoding = tokenizer.batch_encode_plus(\n        texts,\n        # This is required to add special tokens such as the [CLS] and [SEP] tokens that indicate the start and end of a sentence\n        add_special_tokens=True,\n        # Here the padding variable is responsible for padding the sequences to the same length\n        padding='max_length',\n        # The max length of the tokenized sequences\n        max_length=max_length,\n        return_attention_mask=True,\n        # Here we specify that we want the output to be TensorFlow tensors\n        return_tensors='tf',\n        # If the sequence is longer than max_length, it will be truncated to a fixed length\n        truncation=True\n    )\n    # The encoding['input_ids'] contains the tokenized sequences\n    # The encoding['attention_mask'] contains the attention masks and tells the model which tokens to pay attention to and which ones to ignore (mask token)\n    return encoding['input_ids'], encoding['attention_mask']\n\n# Here we define the maximum length (randomly chosen per ChatGPT's recommendation)\nmax_length = 128\n\n# We can then call the function to tokenize and encode the dataset\ninput_ids, attention_masks = encode_texts(tokenizer, df['tweet'].tolist(), max_length)\n\n# Here we create labels from the 'class' column\n# This is the target variable that we want to predict\nlabels = tf.convert_to_tensor(df['class'].values, dtype=tf.int32).numpy()\n\n# For some reason, I was getting an error saying that I needed to convert to NumPy arrays instead of TensorFlow tensors\n# So I converted the input_ids and attention_masks to NumPy arrays\ninput_ids_np = input_ids.numpy()\nattention_masks_np = attention_masks.numpy()\n\n# We can then use the train_test_split function from scikit-learn to split the dataset into training and validation sets\ntrain_inputs_np, validation_inputs_np, train_labels_np, validation_labels_np = train_test_split(input_ids_np, labels, random_state=2021, test_size=0.1)\ntrain_masks_np, validation_masks_np, _, _ = train_test_split(attention_masks_np, labels, random_state=2021, test_size=0.1)\n\n# Here the BUFFER_SIZE is the number of training inputs\nBUFFER_SIZE = len(train_inputs_np)\n# The batch size is 32\nBATCH_SIZE = 32\n\n# We can then take the NumPy arrays and convert them to TensorFlow datasets for both the training and validation sets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(((train_inputs_np, train_masks_np), train_labels_np)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(((validation_inputs_np, validation_masks_np), validation_labels_np)).batch(BATCH_SIZE)","metadata":{"id":"l7VSNqKTF1dG","execution":{"iopub.status.busy":"2024-03-22T18:52:29.503078Z","iopub.execute_input":"2024-03-22T18:52:29.505742Z","iopub.status.idle":"2024-03-22T18:54:15.354505Z","shell.execute_reply.started":"2024-03-22T18:52:29.505711Z","shell.execute_reply":"2024-03-22T18:54:15.353654Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertForSequenceClassification, BertTokenizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\n\ndef train_bert_sequence_classification(train_dataset, validation_dataset, num_epochs=3):\n    # Initialize the BERT model for sequence classification\n    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3, output_attentions=False, output_hidden_states=False)\n\n    # Define the optimizer\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-8)\n\n    # Define the loss function\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    # Compile the model\n    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n    \n    # Train the model\n    model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset)\n\n    # Evaluate the model on training dataset\n    train_loss, train_accuracy = model.evaluate(train_dataset)\n    print(f\"Training Accuracy: {train_accuracy}\")\n    print(f\"Training Loss: {train_loss}\")\n    \n    return model\n\n# Train the BERT model\nmodel = train_bert_sequence_classification(train_dataset, validation_dataset)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MX69CBkhNPNc","outputId":"d90bf98e-ea4f-4c6f-8d02-a7329055e19c","execution":{"iopub.status.busy":"2024-03-22T19:40:20.896649Z","iopub.execute_input":"2024-03-22T19:40:20.897073Z","iopub.status.idle":"2024-03-22T20:00:25.952226Z","shell.execute_reply.started":"2024-03-22T19:40:20.897042Z","shell.execute_reply":"2024-03-22T20:00:25.951291Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n697/697 [==============================] - 414s 501ms/step - loss: 0.4957 - accuracy: 0.8287 - val_loss: 0.4774 - val_accuracy: 0.8217\nEpoch 2/3\n697/697 [==============================] - 338s 484ms/step - loss: 0.4620 - accuracy: 0.8332 - val_loss: 0.4693 - val_accuracy: 0.8217\nEpoch 3/3\n697/697 [==============================] - 338s 484ms/step - loss: 0.4609 - accuracy: 0.8332 - val_loss: 0.4758 - val_accuracy: 0.8217\n697/697 [==============================] - 114s 164ms/step - loss: 0.4616 - accuracy: 0.8332\nTraining Accuracy: 0.8331689238548279\nTraining Loss: 0.46161115169525146\n","output_type":"stream"}]},{"cell_type":"code","source":"val_loss, val_acc = model.evaluate(validation_dataset)\nprint(f\"Val Accuracy: {val_acc}\")\nprint(f\"Val Loss: {val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:05:48.922531Z","iopub.execute_input":"2024-03-22T20:05:48.923282Z","iopub.status.idle":"2024-03-22T20:06:01.680807Z","shell.execute_reply.started":"2024-03-22T20:05:48.923247Z","shell.execute_reply":"2024-03-22T20:06:01.679949Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"78/78 [==============================] - 13s 163ms/step - loss: 0.4758 - accuracy: 0.8217\nVal Accuracy: 0.8217023015022278\nVal Loss: 0.47575679421424866\n","output_type":"stream"}]}]}