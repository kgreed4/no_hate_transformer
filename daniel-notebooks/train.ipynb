{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Encoder  # Import your custom Encoder\n",
    "from dataset import TextClassificationDataset  # Assuming you adapted this class based on previous instructions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model import EncoderBlock, MultiHeadAttentionBlock, FeedForwardBlock, InputEmbeddings, PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize the input text using the BERT tokenizer\n",
    "    \"\"\"\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Tokenize the text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    return tokenizer, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_encoder(config):\n",
    "    \"\"\"\n",
    "    Initialize the Transformer encoder\n",
    "    \"\"\"\n",
    "    # Initialize the Transformer encoder layers (you can adjust the number of layers)\n",
    "    encoder_layers = nn.ModuleList([EncoderBlock(MultiHeadAttentionBlock(config['d_model'], h=config['heads'], dropout=config['dropout_rate']),\n",
    "                                                FeedForwardBlock(d_model=config['d_model'], d_ff=config['d_ff'], dropout=config['dropout_rate']),\n",
    "                                                dropout=config['dropout_rate'])\n",
    "                                    for _ in range(config['num_layers'])])\n",
    "    encoder = Encoder(encoder_layers)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(encoder, input_ids, config):\n",
    "    \"\"\"\n",
    "    Initialize the model, the optimizer and the loss function\n",
    "    \"\"\"\n",
    "    # Create the embedding and positional encoding layers\n",
    "    input_embedding = InputEmbeddings(config['d_model'], config['vocab_size'])\n",
    "    positional_encoding = PositionalEncoding(config['d_model'], config['seq_len'], config['dropout_rate'])\n",
    "\n",
    "    # Encode the input text\n",
    "    # convert the input_ids to embeddings\n",
    "    embeddings = input_embedding(input_ids)\n",
    "    # add the positional encoding to the embeddings\n",
    "    embeddings = positional_encoding(embeddings)\n",
    "    # pass the embeddings through the encoder\n",
    "    encoded_text = encoder(embeddings, None)\n",
    "\n",
    "    return encoded_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7268, -0.0136,  0.0512,  ..., -1.8737, -0.8607, -0.8110],\n",
      "         [ 1.1094, -0.0577, -0.7989,  ..., -0.4510, -0.0106,  0.9494],\n",
      "         [ 0.0223,  0.1444, -1.1096,  ...,  0.0805,  1.1550,  1.3365],\n",
      "         ...,\n",
      "         [-0.7944, -0.0942, -0.3358,  ..., -0.3985, -1.1086,  0.5394],\n",
      "         [-1.3717,  1.6325, -0.1623,  ...,  1.7245,  1.5824, -0.4135],\n",
      "         [-2.6252, -0.4823, -1.0174,  ...,  1.3007,  0.0369, -0.8003]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer, input_ids = tokenize(\"Hello, my dog is cute\")\n",
    "\n",
    "config = {\n",
    "    \"d_model\": 512,\n",
    "    \"seq_len\": 128,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"heads\": 8,\n",
    "    \"d_ff\": 2048,\n",
    "    \"num_layers\": 6\n",
    "}\n",
    "\n",
    "encoder = init_encoder(config)\n",
    "encoded_text = encode_text(encoder, input_ids, config)\n",
    "print(encoded_text)  # Expected output: torch.Size([1, 128, 512])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, encoder, d_model, output_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x, config):\n",
    "        # Encode the input text using the provided encode_text function\n",
    "        encoded_text = encode_text(self.encoder, x, config)\n",
    "        # Use only the encoding of the [CLS] token for classification purposes\n",
    "        cls_encoding = encoded_text[:, 0, :]  # Assuming [CLS] is at the first position\n",
    "        # Pass the [CLS] encoding through the classifier\n",
    "        return self.classifier(cls_encoding)\n",
    "\n",
    "# Configuration for the encoder and classifier\n",
    "config = {\n",
    "    \"d_model\": 512,\n",
    "    \"seq_len\": 128,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"heads\": 8,\n",
    "    \"d_ff\": 2048,\n",
    "    \"num_layers\": 6\n",
    "}\n",
    "\n",
    "# Initialize the encoder and classifier\n",
    "encoder = init_encoder(config)\n",
    "model = TextClassifier(encoder, config['d_model'], 1)  # '1' for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length') for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {'input_ids': torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "                'labels': torch.tensor(self.labels[idx], dtype=torch.float)}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('cleaned_data_sw.csv')\n",
    "# Assuming data is your DataFrame containing 'tweet' and 'class'\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into training and temp data sets (80-20 split)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temp data into validation and test data sets (50-50 split of 20% total data)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now you can create datasets for each split\n",
    "train_dataset = TextDataset(train_data['tweet'].tolist(), train_data['class'].tolist(), tokenizer, config['seq_len'])\n",
    "validation_dataset = TextDataset(validation_data['tweet'].tolist(), validation_data['class'].tolist(), tokenizer, config['seq_len'])\n",
    "test_dataset = TextDataset(test_data['tweet'].tolist(), test_data['class'].tolist(), tokenizer, config['seq_len'])\n",
    "\n",
    "# And DataLoaders for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4555098480394771, Validation Loss: 0.47362861527550604\n",
      "Epoch 2, Train Loss: 0.4554745435414295, Validation Loss: 0.4719776435244468\n",
      "Epoch 3, Train Loss: 0.45463386024198227, Validation Loss: 0.47069941345722444\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "best_val_loss = float('inf')  # Initialize best validation loss for early stopping/checkpointing\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Assuming 'model' is already instantiated\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()  # Use the optimizer instance\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels'].unsqueeze(1)  # Adjust dimensions for consistency\n",
    "        outputs = model(input_ids, config)\n",
    "        loss = loss_fn(outputs, labels)  # Use the loss function instance\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Use the optimizer instance\n",
    "        total_loss += loss.item()\n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            labels = batch['labels'].unsqueeze(1)\n",
    "            outputs = model(input_ids, config)\n",
    "            loss = loss_fn(outputs, labels)  # Use the loss function instance\n",
    "            total_loss += loss.item()\n",
    "    val_loss = total_loss / len(validation_dataloader)\n",
    "\n",
    "    # Print metrics or store them for later\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Check if this is the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming your model architecture is correctly defined as `model`\n",
    "model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "model.to(device)  # Move model to the correct device\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8334\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmedina/Documents/Duke/classes/spring-2024/aipi540/no_hate_transformer/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'test_dataloader' is already instantiated and properly setup\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].unsqueeze(1).to(device)  # Adjust dimensions if necessary\n",
    "        outputs = model(input_ids, config).squeeze()  # Remove extra dimensions from model outputs if necessary\n",
    "        predicted_labels = torch.round(torch.sigmoid(outputs))  # Convert logits to binary predictions\n",
    "        true_labels.extend(labels.detach().cpu().numpy())\n",
    "        predictions.extend(predicted_labels.detach().cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculation\n",
    "true_labels = np.array(true_labels)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
